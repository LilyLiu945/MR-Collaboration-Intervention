"""
01 - HMMå»ºæ¨¡ä¸æ ‡ç­¾ç”Ÿæˆ

æœ¬è„šæœ¬å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. åŠ è½½é¢„å¤„ç†åçš„æ•°æ®
2. è®­ç»ƒç²—ç²’åº¦HMMï¼ˆ4çŠ¶æ€ï¼‰è¯†åˆ«åä½œçŠ¶æ€
3. è®­ç»ƒç»†ç²’åº¦HMMï¼ˆé’ˆå¯¹ä½æ²Ÿé€šçŠ¶æ€ï¼‰
4. é¢„æµ‹æ‰€æœ‰æ•°æ®é›†çš„HMMçŠ¶æ€
5. å°†HMMçŠ¶æ€æ˜ å°„ä¸ºäºŒåˆ†ç±»æ ‡ç­¾ï¼ˆ0=æ— éœ€å¹²é¢„ï¼Œ1=éœ€è¦å¹²é¢„ï¼‰
6. ä¿å­˜æ ‡ç­¾å’ŒHMMæ¨¡å‹

**ä¸“ä¸šåˆ†æ**ï¼š
- HMMç”¨äºå‘ç°æ½œåœ¨çš„åä½œçŠ¶æ€ï¼ˆå¹³è¡¡/ä¸å¹³è¡¡ï¼‰
- ä½æ²Ÿé€šçŠ¶æ€æ˜ å°„ä¸ºéœ€è¦å¹²é¢„ï¼ˆæ ‡ç­¾1ï¼‰
- é«˜æ²Ÿé€šçŠ¶æ€æ˜ å°„ä¸ºæ— éœ€å¹²é¢„ï¼ˆæ ‡ç­¾0ï¼‰
"""

# ============================================================================
# é…ç½®éƒ¨åˆ†
# ============================================================================

import os
from pathlib import Path
import pandas as pd
import numpy as np
import pickle
import warnings
warnings.filterwarnings('ignore')

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent

# è¾“å‡ºè·¯å¾„
OUTPUT_DIR = PROJECT_ROOT / "outputs"
OUTPUT_DIR.mkdir(exist_ok=True)
INTERMEDIATE_DIR = OUTPUT_DIR / "intermediate"
INTERMEDIATE_DIR.mkdir(exist_ok=True)
MODELS_DIR = OUTPUT_DIR / "models"
MODELS_DIR.mkdir(exist_ok=True)
REPORTS_DIR = OUTPUT_DIR / "reports"
REPORTS_DIR.mkdir(exist_ok=True)
VISUALIZATIONS_DIR = OUTPUT_DIR / "visualizations"
VISUALIZATIONS_DIR.mkdir(exist_ok=True)

# HMMé…ç½®
HMM_CONFIG = {
    "coarse_n_states": 4,  # ç²—ç²’åº¦çŠ¶æ€æ•°
    "fine_n_states": 3,  # ç»†ç²’åº¦çŠ¶æ€æ•°
    "n_iter": 100,  # Baum-Welchç®—æ³•è¿­ä»£æ¬¡æ•°
    "covariance_type": "full",  # åæ–¹å·®ç±»å‹
    "random_state": 42,
}

# HMMå¤šåˆ†ç±»é…ç½®
HMM_N_CLASSES = 4  # 3æˆ–4åˆ†ç±»ï¼Œ4è¡¨ç¤ºç›´æ¥ä½¿ç”¨HMMçš„4ä¸ªçŠ¶æ€
HMM_STATE_MAPPING = None  # å¦‚æœn_classes=3ï¼Œå¯ä»¥æŒ‡å®šçŠ¶æ€æ˜ å°„ï¼Œä¾‹å¦‚ï¼š{0: 0, 1: 0, 2: 1, 3: 2}

# éšæœºç§å­
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# ============================================================================
# å·¥å…·å‡½æ•°éƒ¨åˆ†
# ============================================================================

def load_intermediate(name, directory=None):
    """ä»intermediateç›®å½•åŠ è½½ä¸­é—´ç»“æœ"""
    if directory is None:
        directory = INTERMEDIATE_DIR
    filepath = directory / f"{name}.pkl"
    if not filepath.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    with open(filepath, 'rb') as f:
        data = pickle.load(f)
    print(f"å·²åŠ è½½: {filepath}")
    return data


def save_intermediate(name, data, directory=None):
    """ä¿å­˜ä¸­é—´ç»“æœåˆ°intermediateç›®å½•"""
    if directory is None:
        directory = INTERMEDIATE_DIR
    filepath = directory / f"{name}.pkl"
    with open(filepath, 'wb') as f:
        pickle.dump(data, f)
    print(f"å·²ä¿å­˜: {filepath}")


def save_model(name, model, directory=None):
    """ä¿å­˜æ¨¡å‹"""
    if directory is None:
        directory = MODELS_DIR
    filepath = directory / f"{name}.pkl"
    with open(filepath, 'wb') as f:
        pickle.dump(model, f)
    print(f"å·²ä¿å­˜æ¨¡å‹: {filepath}")


try:
    from hmmlearn import hmm
    HMM_AVAILABLE = True
except ImportError:
    HMM_AVAILABLE = False
    print("é”™è¯¯: hmmlearnæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install hmmlearn")
    exit(1)


def train_coarse_hmm(X_train, n_states=4, n_iter=100, covariance_type='full', random_state=42):
    """
    è®­ç»ƒç²—ç²’åº¦HMM
    
    Parameters:
    -----------
    X_train : np.ndarray
        è®­ç»ƒæ•°æ® (n_samples, n_features)
    n_states : int
        çŠ¶æ€æ•°
    n_iter : int
        è¿­ä»£æ¬¡æ•°
    covariance_type : str
        åæ–¹å·®ç±»å‹
    random_state : int
        éšæœºç§å­
    
    Returns:
    --------
    model : hmm.GaussianHMM
        è®­ç»ƒå¥½çš„HMMæ¨¡å‹
    """
    print(f"\nè®­ç»ƒç²—ç²’åº¦HMM ({n_states}çŠ¶æ€)...")
    
    model = hmm.GaussianHMM(
        n_components=n_states,
        covariance_type=covariance_type,
        n_iter=n_iter,
        random_state=random_state,
        verbose=False
    )
    
    model.fit(X_train)
    
    print(f"âœ“ HMMè®­ç»ƒå®Œæˆï¼Œæ”¶æ•›è¿­ä»£æ¬¡æ•°: {model.monitor_.iter}")
    return model


def predict_hmm_states(model, X):
    """
    ä½¿ç”¨HMMé¢„æµ‹çŠ¶æ€åºåˆ—
    
    Parameters:
    -----------
    model : hmm.GaussianHMM
        HMMæ¨¡å‹
    X : np.ndarray
        è§‚æµ‹æ•°æ®
    
    Returns:
    --------
    states : np.ndarray
        é¢„æµ‹çš„çŠ¶æ€åºåˆ—
    """
    states = model.predict(X)
    return states


def map_states_to_labels(states, n_classes=4, state_mapping=None):
    """
    å°†HMMçŠ¶æ€æ˜ å°„ä¸ºå¤šåˆ†ç±»æ ‡ç­¾
    
    ç­–ç•¥ï¼š
    - ç›´æ¥ä½¿ç”¨HMMçš„4ä¸ªçŠ¶æ€ä½œä¸º4åˆ†ç±»æ ‡ç­¾
    - æˆ–è€…æ˜ å°„ä¸º3åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
    
    Parameters:
    -----------
    states : np.ndarray
        HMMçŠ¶æ€åºåˆ—
    n_classes : int
        åˆ†ç±»æ•°é‡ï¼ˆ3æˆ–4ï¼‰
    state_mapping : dict, optional
        çŠ¶æ€æ˜ å°„å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ˜ å°„
        ä¾‹å¦‚ï¼š{0: 0, 1: 1, 2: 2, 3: 2} è¡¨ç¤ºå°†çŠ¶æ€2å’Œ3åˆå¹¶ä¸ºç±»åˆ«2
    
    Returns:
    --------
    labels : np.ndarray
        å¤šåˆ†ç±»æ ‡ç­¾ (0, 1, 2, ...)
    """
    if n_classes == 4:
        # ç›´æ¥ä½¿ç”¨4ä¸ªçŠ¶æ€ä½œä¸º4åˆ†ç±»
        labels = states.copy()
    elif n_classes == 3:
        # æ˜ å°„ä¸º3åˆ†ç±»
        if state_mapping is None:
            # é»˜è®¤æ˜ å°„ï¼šçŠ¶æ€0,1 -> ç±»åˆ«0ï¼ˆä½æ²Ÿé€šï¼‰ï¼ŒçŠ¶æ€2 -> ç±»åˆ«1ï¼ˆä¸­ç­‰ï¼‰ï¼ŒçŠ¶æ€3 -> ç±»åˆ«2ï¼ˆé«˜æ²Ÿé€šï¼‰
            state_mapping = {0: 0, 1: 0, 2: 1, 3: 2}
        labels = np.array([state_mapping[s] for s in states])
    else:
        raise ValueError(f"n_classeså¿…é¡»æ˜¯3æˆ–4ï¼Œå½“å‰ä¸º{n_classes}")
    
    return labels


# ============================================================================
# ä¸»ç¨‹åºéƒ¨åˆ†
# ============================================================================

print("\n" + "="*80)
print("01 - HMMå»ºæ¨¡ä¸æ ‡ç­¾ç”Ÿæˆ")
print("="*80)

# 1. åŠ è½½æ•°æ®
print("\n" + "-"*80)
print("1. åŠ è½½é¢„å¤„ç†åçš„æ•°æ®")
print("-"*80)

train_data = load_intermediate('train_data')
train_val_data = load_intermediate('train_val_data')
val_data = load_intermediate('val_data')
test_data = load_intermediate('test_data')
feature_names = load_intermediate('feature_names')

print(f"\næ•°æ®å½¢çŠ¶:")
print(f"è®­ç»ƒé›†: {train_data.shape}")
print(f"æµ‹è¯•é›†: {test_data.shape}")
print(f"ç‰¹å¾æ•°: {len(feature_names)}")

# æå–ç‰¹å¾åˆ—
exclude_cols = ['group', 'window_idx']
feature_cols = [col for col in train_data.columns if col not in exclude_cols]

X_train = train_data[feature_cols].values
# æ£€æŸ¥ train_val_data æ˜¯å¦ä¸ºç©º
if len(train_val_data) > 0:
    X_train_val = train_val_data[feature_cols].values
else:
    X_train_val = np.array([]).reshape(0, len(feature_cols))
# æ£€æŸ¥ val_data æ˜¯å¦ä¸ºç©ºï¼ˆå·²åˆå¹¶åˆ°æµ‹è¯•é›†ï¼‰
if len(val_data) > 0:
    X_val = val_data[feature_cols].values
else:
    X_val = np.array([]).reshape(0, len(feature_cols))
X_test = test_data[feature_cols].values

print(f"\nç‰¹å¾çŸ©é˜µå½¢çŠ¶:")
print(f"X_train: {X_train.shape}")
print(f"X_test: {X_test.shape}")

# 2. è®­ç»ƒç²—ç²’åº¦HMM
print("\n" + "-"*80)
print("2. è®­ç»ƒç²—ç²’åº¦HMM")
print("-"*80)

coarse_hmm = train_coarse_hmm(
    X_train,
    n_states=HMM_CONFIG['coarse_n_states'],
    n_iter=HMM_CONFIG['n_iter'],
    covariance_type=HMM_CONFIG['covariance_type'],
    random_state=HMM_CONFIG['random_state']
)

# ä¿å­˜æ¨¡å‹
save_model('coarse_hmm', coarse_hmm)

# 3. é¢„æµ‹æ‰€æœ‰æ•°æ®é›†çš„çŠ¶æ€
print("\n" + "-"*80)
print("3. é¢„æµ‹HMMçŠ¶æ€")
print("-"*80)

states_train = predict_hmm_states(coarse_hmm, X_train)
# å¦‚æœ X_train_val ä¸ºç©ºï¼Œè·³è¿‡é¢„æµ‹
if len(X_train_val) > 0:
    states_train_val = predict_hmm_states(coarse_hmm, X_train_val)
else:
    states_train_val = np.array([])
# å¦‚æœ X_val ä¸ºç©ºï¼ˆå·²åˆå¹¶åˆ°æµ‹è¯•é›†ï¼‰ï¼Œè·³è¿‡é¢„æµ‹
if len(X_val) > 0:
    states_val = predict_hmm_states(coarse_hmm, X_val)
else:
    states_val = np.array([])
states_test = predict_hmm_states(coarse_hmm, X_test)

print(f"\nçŠ¶æ€åˆ†å¸ƒ:")
print(f"è®­ç»ƒé›†: {pd.Series(states_train).value_counts().sort_index().to_dict()}")
print(f"æµ‹è¯•é›†: {pd.Series(states_test).value_counts().sort_index().to_dict()}")

# 3.1 åˆ†æHMMçŠ¶æ€çš„è¯­ä¹‰å«ä¹‰
print("\n" + "-"*80)
print("3.1 åˆ†æHMMçŠ¶æ€çš„è¯­ä¹‰å«ä¹‰")
print("-"*80)
print("\nâš ï¸  é‡è¦ï¼šHMMçŠ¶æ€æœ¬èº«æ²¡æœ‰é¢„è®¾è¯­ä¹‰ï¼Œéœ€è¦æ ¹æ®ç‰¹å¾å€¼åˆ†æå…¶å«ä¹‰")
print("ä»¥ä¸‹åˆ†ææ¯ä¸ªçŠ¶æ€çš„å¹³å‡ç‰¹å¾å€¼ï¼Œå¸®åŠ©ç†è§£çŠ¶æ€å«ä¹‰ï¼š\n")

# åˆ›å»ºçŠ¶æ€åˆ†æDataFrame
state_analysis = pd.DataFrame(index=feature_cols)
state_means = {}  # å­˜å‚¨æ¯ä¸ªçŠ¶æ€çš„ç‰¹å¾å‡å€¼

for state in range(HMM_CONFIG['coarse_n_states']):
    # è·å–è¯¥çŠ¶æ€çš„æ‰€æœ‰æ ·æœ¬
    state_mask = states_train == state
    if np.sum(state_mask) > 0:
        state_features = X_train[state_mask]
        state_mean = np.mean(state_features, axis=0)
        state_means[state] = state_mean
        state_analysis[f'çŠ¶æ€{state}_å‡å€¼'] = state_mean
        state_analysis[f'çŠ¶æ€{state}_æ ·æœ¬æ•°'] = np.sum(state_mask)
    else:
        state_means[state] = np.zeros(len(feature_cols))
        state_analysis[f'çŠ¶æ€{state}_å‡å€¼'] = np.zeros(len(feature_cols))
        state_analysis[f'çŠ¶æ€{state}_æ ·æœ¬æ•°'] = 0

# æ˜¾ç¤ºå…³é”®ç‰¹å¾çš„åˆ†æï¼ˆé€‰æ‹©ä¸€äº›æœ‰ä»£è¡¨æ€§çš„ç‰¹å¾ï¼‰
print("="*80)
print("å…³é”®ç‰¹å¾åœ¨å„çŠ¶æ€ä¸‹çš„å¹³å‡å€¼ï¼ˆå¸®åŠ©ç†è§£çŠ¶æ€å«ä¹‰ï¼‰:")
print("="*80)

# é€‰æ‹©å…³é”®ç‰¹å¾ï¼ˆåŒ…å«density, clustering, eigenvector, reciprocityç­‰ï¼‰
key_features = [f for f in feature_cols if any(keyword in f.lower() for keyword in 
                ['density', 'clustering', 'eigenvector', 'reciprocity', 'betweenness', 'degree', 'closeness'])]

if len(key_features) > 0:
    print(f"\nå…³é”®ç‰¹å¾åˆ†æï¼ˆå…±{len(key_features)}ä¸ªï¼‰:")
    key_analysis = state_analysis.loc[key_features]
    print(key_analysis.to_string())
else:
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³é”®ç‰¹å¾ï¼Œæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾
    print(f"\nå‰10ä¸ªç‰¹å¾çš„åˆ†æ:")
    print(state_analysis.head(10).to_string())

# åˆ†ææ¯ä¸ªçŠ¶æ€çš„ç‰¹å¾æ¨¡å¼
print("\n" + "="*80)
print("çŠ¶æ€ç‰¹å¾æ¨¡å¼åˆ†æï¼ˆå¸®åŠ©åˆ¤æ–­å“ªä¸ªçŠ¶æ€æ˜¯'ä½æ²Ÿé€š'/'é«˜æ²Ÿé€š'ï¼‰:")
print("="*80)

for state in range(HMM_CONFIG['coarse_n_states']):
    print(f"\nçŠ¶æ€{state}:")
    print(f"  æ ·æœ¬æ•°: {int(state_analysis[f'çŠ¶æ€{state}_æ ·æœ¬æ•°'].iloc[0])}")
    
    # åˆ†æå…³é”®æŒ‡æ ‡
    state_mean = state_means[state]
    feature_dict = dict(zip(feature_cols, state_mean))
    
    # æŸ¥æ‰¾densityç›¸å…³çš„ç‰¹å¾ï¼ˆé€šå¸¸densityé«˜è¡¨ç¤ºæ²Ÿé€šé¢‘ç¹ï¼‰
    density_features = {k: v for k, v in feature_dict.items() if 'density' in k.lower()}
    if density_features:
        avg_density = np.mean(list(density_features.values()))
        print(f"  å¹³å‡Density: {avg_density:.4f} (é«˜å€¼å¯èƒ½è¡¨ç¤ºæ²Ÿé€šé¢‘ç¹)")
    
    # æŸ¥æ‰¾clusteringç›¸å…³çš„ç‰¹å¾
    clustering_features = {k: v for k, v in feature_dict.items() if 'clustering' in k.lower()}
    if clustering_features:
        avg_clustering = np.mean(list(clustering_features.values()))
        print(f"  å¹³å‡Clustering: {avg_clustering:.4f} (é«˜å€¼å¯èƒ½è¡¨ç¤ºåä½œç´§å¯†)")
    
    # æŸ¥æ‰¾eigenvectorç›¸å…³çš„ç‰¹å¾
    eigenvector_features = {k: v for k, v in feature_dict.items() if 'eigenvector' in k.lower()}
    if eigenvector_features:
        avg_eigenvector = np.mean(list(eigenvector_features.values()))
        print(f"  å¹³å‡Eigenvector: {avg_eigenvector:.4f} (é«˜å€¼å¯èƒ½è¡¨ç¤ºå½±å“åŠ›å¤§)")

print("\n" + "="*80)
print("ğŸ’¡ å»ºè®®ï¼š")
print("  1. æŸ¥çœ‹ä¸Šè¿°ç‰¹å¾å€¼ï¼Œåˆ¤æ–­å“ªä¸ªçŠ¶æ€çš„ç‰¹å¾å€¼è¾ƒä½ï¼ˆå¯èƒ½æ˜¯'ä½æ²Ÿé€š'çŠ¶æ€ï¼‰")
print("  2. åˆ¤æ–­å“ªä¸ªçŠ¶æ€çš„ç‰¹å¾å€¼è¾ƒé«˜ï¼ˆå¯èƒ½æ˜¯'é«˜æ²Ÿé€š'çŠ¶æ€ï¼‰")
print("  3. æ ¹æ®åˆ†æç»“æœï¼Œåœ¨ä¸‹é¢çš„é…ç½®ä¸­è®¾ç½® HMM_STATE_MAPPING")
print("  4. ä¾‹å¦‚ï¼šå¦‚æœçŠ¶æ€0æ˜¯ä½æ²Ÿé€šï¼ŒçŠ¶æ€3æ˜¯é«˜æ²Ÿé€šï¼Œå¯ä»¥è®¾ç½®ï¼š")
print("     HMM_STATE_MAPPING = {0: 0, 1: 1, 2: 2, 3: 3}  # ä¿æŒ4åˆ†ç±»")
print("     æˆ–è€…æ˜ å°„ä¸º3åˆ†ç±»ï¼š{0: 0, 1: 0, 2: 1, 3: 2}  # ä½æ²Ÿé€š->0, ä¸­ç­‰->1, é«˜æ²Ÿé€š->2")
print("="*80)

# ä¿å­˜çŠ¶æ€åˆ†æç»“æœ
state_analysis_path = REPORTS_DIR / "hmm_state_analysis.csv"
state_analysis.to_csv(state_analysis_path, encoding='utf-8')
print(f"\nâœ“ çŠ¶æ€åˆ†æç»“æœå·²ä¿å­˜åˆ°: {state_analysis_path}")
print("  å¯ä»¥æ‰“å¼€CSVæ–‡ä»¶æŸ¥çœ‹æ‰€æœ‰ç‰¹å¾åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹çš„è¯¦ç»†å€¼")

# 4. æ˜ å°„çŠ¶æ€åˆ°æ ‡ç­¾
print("\n" + "-"*80)
print("4. æ˜ å°„HMMçŠ¶æ€åˆ°å¤šåˆ†ç±»æ ‡ç­¾")
print("-"*80)

# HMMå¤šåˆ†ç±»é…ç½®
# âš ï¸  é‡è¦ï¼šæ ¹æ®ä¸Šé¢çš„çŠ¶æ€åˆ†æç»“æœï¼Œè®¾ç½®çŠ¶æ€æ˜ å°„
# å¦‚æœä¿æŒ4åˆ†ç±»ï¼Œç›´æ¥ä½¿ç”¨HMMçš„4ä¸ªçŠ¶æ€ï¼šHMM_STATE_MAPPING = None
# å¦‚æœæ˜ å°„ä¸º3åˆ†ç±»ï¼Œä¾‹å¦‚ï¼š{0: 0, 1: 0, 2: 1, 3: 2} è¡¨ç¤ºçŠ¶æ€0,1->ç±»åˆ«0ï¼ˆä½æ²Ÿé€šï¼‰ï¼ŒçŠ¶æ€2->ç±»åˆ«1ï¼ˆä¸­ç­‰ï¼‰ï¼ŒçŠ¶æ€3->ç±»åˆ«2ï¼ˆé«˜æ²Ÿé€šï¼‰
HMM_N_CLASSES = 4  # 3æˆ–4åˆ†ç±»ï¼Œ4è¡¨ç¤ºç›´æ¥ä½¿ç”¨HMMçš„4ä¸ªçŠ¶æ€
HMM_STATE_MAPPING = None  # å¦‚æœn_classes=3ï¼Œå¯ä»¥æŒ‡å®šçŠ¶æ€æ˜ å°„ï¼Œä¾‹å¦‚ï¼š{0: 0, 1: 0, 2: 1, 3: 2}

print(f"\nä½¿ç”¨HMMçŠ¶æ€ä½œä¸º{HMM_N_CLASSES}åˆ†ç±»æ ‡ç­¾")
if HMM_N_CLASSES == 3:
    print(f"çŠ¶æ€æ˜ å°„: {HMM_STATE_MAPPING if HMM_STATE_MAPPING else 'é»˜è®¤æ˜ å°„ï¼ˆ0,1->0, 2->1, 3->2ï¼‰'}")
elif HMM_N_CLASSES == 4:
    if HMM_STATE_MAPPING is None:
        print("ç›´æ¥ä½¿ç”¨HMMçš„4ä¸ªçŠ¶æ€ï¼ˆ0,1,2,3ï¼‰ä½œä¸º4ä¸ªç±»åˆ«")
        print("âš ï¸  æ³¨æ„ï¼šç±»åˆ«0,1,2,3æ²¡æœ‰é¢„è®¾è¯­ä¹‰ï¼Œéœ€è¦æ ¹æ®ä¸Šé¢çš„çŠ¶æ€åˆ†æç»“æœç†è§£å…¶å«ä¹‰")
    else:
        print(f"çŠ¶æ€æ˜ å°„: {HMM_STATE_MAPPING}")

y_train = map_states_to_labels(states_train, n_classes=HMM_N_CLASSES, state_mapping=HMM_STATE_MAPPING)
# å¦‚æœ states_train_val ä¸ºç©ºï¼Œåˆ›å»ºç©ºæ•°ç»„
if len(states_train_val) > 0:
    y_train_val = map_states_to_labels(states_train_val, n_classes=HMM_N_CLASSES, state_mapping=HMM_STATE_MAPPING)
else:
    y_train_val = np.array([])
# å¦‚æœ states_val ä¸ºç©ºï¼ˆå·²åˆå¹¶åˆ°æµ‹è¯•é›†ï¼‰ï¼Œåˆ›å»ºç©ºæ•°ç»„
if len(states_val) > 0:
    y_val = map_states_to_labels(states_val, n_classes=HMM_N_CLASSES, state_mapping=HMM_STATE_MAPPING)
else:
    y_val = np.array([])
y_test = map_states_to_labels(states_test, n_classes=HMM_N_CLASSES, state_mapping=HMM_STATE_MAPPING)

print(f"\næ ‡ç­¾åˆ†å¸ƒï¼ˆ{HMM_N_CLASSES}åˆ†ç±»ï¼‰:")
if HMM_N_CLASSES == 4:
    print(f"è®­ç»ƒé›† - æ ‡ç­¾0: {np.sum(y_train == 0)}, æ ‡ç­¾1: {np.sum(y_train == 1)}, æ ‡ç­¾2: {np.sum(y_train == 2)}, æ ‡ç­¾3: {np.sum(y_train == 3)}")
    print(f"æµ‹è¯•é›† - æ ‡ç­¾0: {np.sum(y_test == 0)}, æ ‡ç­¾1: {np.sum(y_test == 1)}, æ ‡ç­¾2: {np.sum(y_test == 2)}, æ ‡ç­¾3: {np.sum(y_test == 3)}")
else:  # 3åˆ†ç±»
    print(f"è®­ç»ƒé›† - æ ‡ç­¾0: {np.sum(y_train == 0)}, æ ‡ç­¾1: {np.sum(y_train == 1)}, æ ‡ç­¾2: {np.sum(y_train == 2)}")
    print(f"æµ‹è¯•é›† - æ ‡ç­¾0: {np.sum(y_test == 0)}, æ ‡ç­¾1: {np.sum(y_test == 1)}, æ ‡ç­¾2: {np.sum(y_test == 2)}")

# 5. ä¿å­˜ç»“æœ
print("\n" + "-"*80)
print("5. ä¿å­˜ç»“æœ")
print("-"*80)

save_intermediate('states_train', states_train)
if len(states_train_val) > 0:
    save_intermediate('states_train_val', states_train_val)
else:
    save_intermediate('states_train_val', np.array([]))
# ä¿å­˜ç©ºçš„éªŒè¯é›†çŠ¶æ€ï¼ˆå·²åˆå¹¶åˆ°æµ‹è¯•é›†ï¼‰
if len(states_val) > 0:
    save_intermediate('states_val', states_val)
else:
    save_intermediate('states_val', np.array([]))
save_intermediate('states_test', states_test)

save_intermediate('y_train', y_train)
if len(y_train_val) > 0:
    save_intermediate('y_train_val', y_train_val)
else:
    save_intermediate('y_train_val', np.array([]))
# ä¿å­˜ç©ºçš„éªŒè¯é›†æ ‡ç­¾ï¼ˆå·²åˆå¹¶åˆ°æµ‹è¯•é›†ï¼‰
if len(y_val) > 0:
    save_intermediate('y_val', y_val)
else:
    save_intermediate('y_val', np.array([]))
save_intermediate('y_test', y_test)

# 6. ç”ŸæˆæŠ¥å‘Š
print("\n" + "-"*80)
print("6. ç”ŸæˆHMMåˆ†ææŠ¥å‘Š")
print("-"*80)

report_lines = []
report_lines.append("=" * 60)
report_lines.append("HMMå»ºæ¨¡æŠ¥å‘Š")
report_lines.append("=" * 60)
report_lines.append(f"\nHMMé…ç½®:")
report_lines.append(f"  çŠ¶æ€æ•°: {HMM_CONFIG['coarse_n_states']}")
report_lines.append(f"  è¿­ä»£æ¬¡æ•°: {HMM_CONFIG['n_iter']}")
report_lines.append(f"  åæ–¹å·®ç±»å‹: {HMM_CONFIG['covariance_type']}")
report_lines.append(f"\nçŠ¶æ€åˆ†å¸ƒ:")
report_lines.append(f"  è®­ç»ƒé›†: {pd.Series(states_train).value_counts().sort_index().to_dict()}")
report_lines.append(f"  æµ‹è¯•é›†: {pd.Series(states_test).value_counts().sort_index().to_dict()}")
report_lines.append(f"\nçŠ¶æ€è¯­ä¹‰åˆ†æ:")
report_lines.append(f"  è¯¦ç»†çš„çŠ¶æ€ç‰¹å¾åˆ†æå·²ä¿å­˜åˆ°: hmm_state_analysis.csv")
report_lines.append(f"  è¯·æŸ¥çœ‹è¯¥æ–‡ä»¶äº†è§£æ¯ä¸ªçŠ¶æ€çš„ç‰¹å¾å€¼ï¼Œåˆ¤æ–­çŠ¶æ€å«ä¹‰")
report_lines.append(f"\næ ‡ç­¾åˆ†å¸ƒï¼ˆ{HMM_N_CLASSES}åˆ†ç±»ï¼‰:")
if HMM_N_CLASSES == 4:
    report_lines.append(f"  è®­ç»ƒé›† - æ ‡ç­¾0: {np.sum(y_train == 0)}, æ ‡ç­¾1: {np.sum(y_train == 1)}, æ ‡ç­¾2: {np.sum(y_train == 2)}, æ ‡ç­¾3: {np.sum(y_train == 3)}")
    report_lines.append(f"  æµ‹è¯•é›† - æ ‡ç­¾0: {np.sum(y_test == 0)}, æ ‡ç­¾1: {np.sum(y_test == 1)}, æ ‡ç­¾2: {np.sum(y_test == 2)}, æ ‡ç­¾3: {np.sum(y_test == 3)}")
else:  # 3åˆ†ç±»
    report_lines.append(f"  è®­ç»ƒé›† - æ ‡ç­¾0: {np.sum(y_train == 0)}, æ ‡ç­¾1: {np.sum(y_train == 1)}, æ ‡ç­¾2: {np.sum(y_train == 2)}")
    report_lines.append(f"  æµ‹è¯•é›† - æ ‡ç­¾0: {np.sum(y_test == 0)}, æ ‡ç­¾1: {np.sum(y_test == 1)}, æ ‡ç­¾2: {np.sum(y_test == 2)}")
if HMM_N_CLASSES == 3:
    report_lines.append(f"\nçŠ¶æ€æ˜ å°„: {HMM_STATE_MAPPING if HMM_STATE_MAPPING else 'é»˜è®¤æ˜ å°„ï¼ˆ0,1->0, 2->1, 3->2ï¼‰'}")

report_text = "\n".join(report_lines)
print(report_text)

with open(REPORTS_DIR / "hmm_analysis_report.txt", 'w', encoding='utf-8') as f:
    f.write(report_text)

print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜åˆ° {REPORTS_DIR / 'hmm_analysis_report.txt'}")

print("\n" + "="*80)
print("HMMå»ºæ¨¡å®Œæˆï¼")
print("="*80)
print("\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œ `02_supervised_feature_selection.py` è¿›è¡Œæœ‰ç›‘ç£ç‰¹å¾é€‰æ‹©")

