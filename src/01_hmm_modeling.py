"""
01 - HMMå»ºæ¨¡ä¸æ ‡ç­¾ç”Ÿæˆ

æœ¬è„šæœ¬å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. åŠ è½½é¢„å¤„ç†åçš„æ•°æ®
2. è®­ç»ƒç²—ç²’åº¦HMMï¼ˆ4çŠ¶æ€ï¼‰è¯†åˆ«åä½œçŠ¶æ€
3. è®­ç»ƒç»†ç²’åº¦HMMï¼ˆé’ˆå¯¹ä½æ²Ÿé€šçŠ¶æ€ï¼‰
4. é¢„æµ‹æ‰€æœ‰æ•°æ®é›†çš„HMMçŠ¶æ€
5. å°†HMMçŠ¶æ€æ˜ å°„ä¸ºäºŒåˆ†ç±»æ ‡ç­¾ï¼ˆ0=æ— éœ€å¹²é¢„ï¼Œ1=éœ€è¦å¹²é¢„ï¼‰
6. ä¿å­˜æ ‡ç­¾å’ŒHMMæ¨¡å‹

**ä¸“ä¸šåˆ†æ**ï¼š
- HMMç”¨äºå‘ç°æ½œåœ¨çš„åä½œçŠ¶æ€ï¼ˆå¹³è¡¡/ä¸å¹³è¡¡ï¼‰
- ä½æ²Ÿé€šçŠ¶æ€æ˜ å°„ä¸ºéœ€è¦å¹²é¢„ï¼ˆæ ‡ç­¾1ï¼‰
- é«˜æ²Ÿé€šçŠ¶æ€æ˜ å°„ä¸ºæ— éœ€å¹²é¢„ï¼ˆæ ‡ç­¾0ï¼‰
"""

# ============================================================================
# é…ç½®éƒ¨åˆ†
# ============================================================================

import os
from pathlib import Path
import pandas as pd
import numpy as np
import pickle
import warnings
warnings.filterwarnings('ignore')

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent

# è¾“å‡ºè·¯å¾„
OUTPUT_DIR = PROJECT_ROOT / "outputs"
OUTPUT_DIR.mkdir(exist_ok=True)
INTERMEDIATE_DIR = OUTPUT_DIR / "intermediate"
INTERMEDIATE_DIR.mkdir(exist_ok=True)
MODELS_DIR = OUTPUT_DIR / "models"
MODELS_DIR.mkdir(exist_ok=True)
REPORTS_DIR = OUTPUT_DIR / "reports"
REPORTS_DIR.mkdir(exist_ok=True)
VISUALIZATIONS_DIR = OUTPUT_DIR / "visualizations"
VISUALIZATIONS_DIR.mkdir(exist_ok=True)

# HMMé…ç½®
HMM_CONFIG = {
    "coarse_n_states": 4,  # ç²—ç²’åº¦çŠ¶æ€æ•°
    "fine_n_states": 3,  # ç»†ç²’åº¦çŠ¶æ€æ•°
    "n_iter": 100,  # Baum-Welchç®—æ³•è¿­ä»£æ¬¡æ•°
    "covariance_type": "full",  # åæ–¹å·®ç±»å‹
    "random_state": 42,
}

# HMMå¤šåˆ†ç±»é…ç½®
HMM_N_CLASSES = 4  # 3æˆ–4åˆ†ç±»ï¼Œ4è¡¨ç¤ºç›´æ¥ä½¿ç”¨HMMçš„4ä¸ªçŠ¶æ€
HMM_STATE_MAPPING = None  # å¦‚æœn_classes=3ï¼Œå¯ä»¥æŒ‡å®šçŠ¶æ€æ˜ å°„ï¼Œä¾‹å¦‚ï¼š{0: 0, 1: 0, 2: 1, 3: 2}

# éšæœºç§å­
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# ============================================================================
# å·¥å…·å‡½æ•°éƒ¨åˆ†
# ============================================================================

def load_intermediate(name, directory=None):
    """ä»intermediateç›®å½•åŠ è½½ä¸­é—´ç»“æœ"""
    if directory is None:
        directory = INTERMEDIATE_DIR
    filepath = directory / f"{name}.pkl"
    if not filepath.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    with open(filepath, 'rb') as f:
        data = pickle.load(f)
    print(f"Loaded: {filepath}")
    return data


def save_intermediate(name, data, directory=None):
    """ä¿å­˜ä¸­é—´ç»“æœåˆ°intermediateç›®å½•"""
    if directory is None:
        directory = INTERMEDIATE_DIR
    filepath = directory / f"{name}.pkl"
    with open(filepath, 'wb') as f:
        pickle.dump(data, f)
    print(f"Saved: {filepath}")


def save_model(name, model, directory=None):
    """ä¿å­˜æ¨¡å‹"""
    if directory is None:
        directory = MODELS_DIR
    filepath = directory / f"{name}.pkl"
    with open(filepath, 'wb') as f:
        pickle.dump(model, f)
    print(f"Model saved: {filepath}")


try:
    from hmmlearn import hmm
    HMM_AVAILABLE = True
except ImportError:
    HMM_AVAILABLE = False
    print("Error: hmmlearn not installed, please run: pip install hmmlearn")
    exit(1)


def train_coarse_hmm(X_train, n_states=4, n_iter=100, covariance_type='full', random_state=42):
    """
    è®­ç»ƒç²—ç²’åº¦HMM
    
    Parameters:
    -----------
    X_train : np.ndarray
        è®­ç»ƒæ•°æ® (n_samples, n_features)
    n_states : int
        çŠ¶æ€æ•°
    n_iter : int
        è¿­ä»£æ¬¡æ•°
    covariance_type : str
        åæ–¹å·®ç±»å‹
    random_state : int
        éšæœºç§å­
    
    Returns:
    --------
    model : hmm.GaussianHMM
        è®­ç»ƒå¥½çš„HMMæ¨¡å‹
    """
    print(f"\nTraining coarse-grained HMM ({n_states} states)...")
    
    model = hmm.GaussianHMM(
        n_components=n_states,
        covariance_type=covariance_type,
        n_iter=n_iter,
        random_state=random_state,
        verbose=False
    )
    
    model.fit(X_train)
    
    print(f"âœ“ HMM training completed, convergence iterations: {model.monitor_.iter}")
    return model


def predict_hmm_states(model, X):
    """
    ä½¿ç”¨HMMé¢„æµ‹çŠ¶æ€åºåˆ—
    
    Parameters:
    -----------
    model : hmm.GaussianHMM
        HMMæ¨¡å‹
    X : np.ndarray
        è§‚æµ‹æ•°æ®
    
    Returns:
    --------
    states : np.ndarray
        é¢„æµ‹çš„çŠ¶æ€åºåˆ—
    """
    states = model.predict(X)
    return states


def map_states_to_labels(states, n_classes=4, state_mapping=None):
    """
    å°†HMMçŠ¶æ€æ˜ å°„ä¸ºå¤šåˆ†ç±»æ ‡ç­¾
    
    ç­–ç•¥ï¼š
    - ç›´æ¥ä½¿ç”¨HMMçš„4ä¸ªçŠ¶æ€ä½œä¸º4åˆ†ç±»æ ‡ç­¾
    - æˆ–è€…æ˜ å°„ä¸º2åˆ†ç±»ï¼ˆéœ€è¦å¹²é¢„ vs ä¸éœ€è¦å¹²é¢„ï¼‰
    - æˆ–è€…æ˜ å°„ä¸º3åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
    
    Parameters:
    -----------
    states : np.ndarray
        HMMçŠ¶æ€åºåˆ—
    n_classes : int
        åˆ†ç±»æ•°é‡ï¼ˆ2, 3æˆ–4ï¼‰
    state_mapping : dict, optional
        çŠ¶æ€æ˜ å°„å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ˜ å°„
        ä¾‹å¦‚ï¼š
        - 2åˆ†ç±»ï¼š{0: 1, 1: 1, 2: 0, 3: 0} è¡¨ç¤ºçŠ¶æ€0,1éœ€è¦å¹²é¢„(1)ï¼ŒçŠ¶æ€2,3ä¸éœ€è¦å¹²é¢„(0)
        - 3åˆ†ç±»ï¼š{0: 0, 1: 0, 2: 1, 3: 2} è¡¨ç¤ºçŠ¶æ€0,1->ç±»åˆ«0ï¼ˆä½æ²Ÿé€šï¼‰ï¼ŒçŠ¶æ€2->ç±»åˆ«1ï¼ˆä¸­ç­‰ï¼‰ï¼ŒçŠ¶æ€3->ç±»åˆ«2ï¼ˆé«˜æ²Ÿé€šï¼‰
    
    Returns:
    --------
    labels : np.ndarray
        å¤šåˆ†ç±»æ ‡ç­¾ (0, 1, 2, ...)
    """
    if n_classes == 4:
        # ç›´æ¥ä½¿ç”¨4ä¸ªçŠ¶æ€ä½œä¸º4åˆ†ç±»
        labels = states.copy()
    elif n_classes == 2:
        # æ˜ å°„ä¸º2åˆ†ç±»ï¼ˆ0=ä¸éœ€è¦å¹²é¢„ï¼Œ1=éœ€è¦å¹²é¢„ï¼‰
        if state_mapping is None:
            # é»˜è®¤æ˜ å°„ï¼šçŠ¶æ€0,1 -> éœ€è¦å¹²é¢„(1)ï¼ŒçŠ¶æ€2,3 -> ä¸éœ€è¦å¹²é¢„(0)
            # è¿™ä¸ªæ˜ å°„ä¼šåœ¨çŠ¶æ€åˆ†æåæ ¹æ®å¥åº·åº¦åˆ†æ•°è‡ªåŠ¨ç”Ÿæˆ
            state_mapping = {0: 1, 1: 1, 2: 0, 3: 0}
        labels = np.array([state_mapping[s] for s in states])
    elif n_classes == 3:
        # æ˜ å°„ä¸º3åˆ†ç±»
        if state_mapping is None:
            # é»˜è®¤æ˜ å°„ï¼šçŠ¶æ€0,1 -> ç±»åˆ«0ï¼ˆä½æ²Ÿé€šï¼‰ï¼ŒçŠ¶æ€2 -> ç±»åˆ«1ï¼ˆä¸­ç­‰ï¼‰ï¼ŒçŠ¶æ€3 -> ç±»åˆ«2ï¼ˆé«˜æ²Ÿé€šï¼‰
            state_mapping = {0: 0, 1: 0, 2: 1, 3: 2}
        labels = np.array([state_mapping[s] for s in states])
    else:
        raise ValueError(f"n_classeså¿…é¡»æ˜¯2ã€3æˆ–4ï¼Œå½“å‰ä¸º{n_classes}")
    
    return labels


# ============================================================================
# ä¸»ç¨‹åºéƒ¨åˆ†
# ============================================================================

print("\n" + "="*80)
print("01 - HMM Modeling and Label Generation")
print("="*80)

# 1. åŠ è½½æ•°æ®
print("\n" + "-"*80)
print("1. Load Preprocessed Data")
print("-"*80)

train_data = load_intermediate('train_data')
train_val_data = load_intermediate('train_val_data')
val_data = load_intermediate('val_data')
test_data = load_intermediate('test_data')
feature_names = load_intermediate('feature_names')

print(f"\nData shapes:")
print(f"Training set: {train_data.shape}")
print(f"Test set: {test_data.shape}")
print(f"Number of features: {len(feature_names)}")

# æå–ç‰¹å¾åˆ—
exclude_cols = ['group', 'window_idx']
feature_cols = [col for col in train_data.columns if col not in exclude_cols]

X_train = train_data[feature_cols].values
# æ£€æŸ¥ train_val_data æ˜¯å¦ä¸ºç©º
if len(train_val_data) > 0:
    X_train_val = train_val_data[feature_cols].values
else:
    X_train_val = np.array([]).reshape(0, len(feature_cols))
# æ£€æŸ¥ val_data æ˜¯å¦ä¸ºç©ºï¼ˆå·²åˆå¹¶åˆ°æµ‹è¯•é›†ï¼‰
if len(val_data) > 0:
    X_val = val_data[feature_cols].values
else:
    X_val = np.array([]).reshape(0, len(feature_cols))
X_test = test_data[feature_cols].values

print(f"\nFeature matrix shapes:")
print(f"X_train: {X_train.shape}")
print(f"X_test: {X_test.shape}")

# 2. è®­ç»ƒç²—ç²’åº¦HMM
print("\n" + "-"*80)
print("2. Train Coarse-grained HMM")
print("-"*80)

coarse_hmm = train_coarse_hmm(
    X_train,
    n_states=HMM_CONFIG['coarse_n_states'],
    n_iter=HMM_CONFIG['n_iter'],
    covariance_type=HMM_CONFIG['covariance_type'],
    random_state=HMM_CONFIG['random_state']
)

# ä¿å­˜æ¨¡å‹
save_model('coarse_hmm', coarse_hmm)

# 3. é¢„æµ‹æ‰€æœ‰æ•°æ®é›†çš„çŠ¶æ€
print("\n" + "-"*80)
print("3. Predict HMM States")
print("-"*80)

states_train = predict_hmm_states(coarse_hmm, X_train)
# å¦‚æœ X_train_val ä¸ºç©ºï¼Œè·³è¿‡é¢„æµ‹
if len(X_train_val) > 0:
    states_train_val = predict_hmm_states(coarse_hmm, X_train_val)
else:
    states_train_val = np.array([])
# å¦‚æœ X_val ä¸ºç©ºï¼ˆå·²åˆå¹¶åˆ°æµ‹è¯•é›†ï¼‰ï¼Œè·³è¿‡é¢„æµ‹
if len(X_val) > 0:
    states_val = predict_hmm_states(coarse_hmm, X_val)
else:
    states_val = np.array([])
states_test = predict_hmm_states(coarse_hmm, X_test)

print(f"\nState distribution:")
print(f"Training set: {pd.Series(states_train).value_counts().sort_index().to_dict()}")
print(f"Test set: {pd.Series(states_test).value_counts().sort_index().to_dict()}")

# 3.1 åˆ†æHMMçŠ¶æ€çš„è¯­ä¹‰å«ä¹‰
print("\n" + "-"*80)
print("3.1 Analyze HMM State Semantics")
print("-"*80)
print("\nâš ï¸  Important: HMM states have no preset semantics, need to analyze their meaning based on feature values")
print("The following analyzes the mean feature values for each state to help understand state meanings:\n")

# åˆ›å»ºçŠ¶æ€åˆ†æDataFrame
state_analysis = pd.DataFrame(index=feature_cols)
state_means = {}  # å­˜å‚¨æ¯ä¸ªçŠ¶æ€çš„ç‰¹å¾å‡å€¼

for state in range(HMM_CONFIG['coarse_n_states']):
    # è·å–è¯¥çŠ¶æ€çš„æ‰€æœ‰æ ·æœ¬
    state_mask = states_train == state
    if np.sum(state_mask) > 0:
        state_features = X_train[state_mask]
        state_mean = np.mean(state_features, axis=0)
        state_means[state] = state_mean
        state_analysis[f'State_{state}_Mean'] = state_mean
        state_analysis[f'State_{state}_Count'] = np.sum(state_mask)
    else:
        state_means[state] = np.zeros(len(feature_cols))
        state_analysis[f'State_{state}_Mean'] = np.zeros(len(feature_cols))
        state_analysis[f'State_{state}_Count'] = 0

# æ˜¾ç¤ºå…³é”®ç‰¹å¾çš„åˆ†æï¼ˆé€‰æ‹©ä¸€äº›æœ‰ä»£è¡¨æ€§çš„ç‰¹å¾ï¼‰
print("="*80)
print("Mean Values of Key Features Across States (Help Understand State Meanings):")
print("="*80)

# é€‰æ‹©å…³é”®ç‰¹å¾ï¼ˆåŒ…å«density, clustering, eigenvector, reciprocityç­‰ï¼‰
key_features = [f for f in feature_cols if any(keyword in f.lower() for keyword in 
                ['density', 'clustering', 'eigenvector', 'reciprocity', 'betweenness', 'degree', 'closeness'])]

if len(key_features) > 0:
    print(f"\nKey feature analysis (total {len(key_features)} features):")
    key_analysis = state_analysis.loc[key_features]
    print(key_analysis.to_string())
else:
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³é”®ç‰¹å¾ï¼Œæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾
    print(f"\nAnalysis of first 10 features:")
    print(state_analysis.head(10).to_string())

# åˆ†ææ¯ä¸ªçŠ¶æ€çš„ç‰¹å¾æ¨¡å¼
print("\n" + "="*80)
print("State Feature Pattern Analysis (Help Determine Which State is 'Low Communication'/'High Communication'):")
print("="*80)

for state in range(HMM_CONFIG['coarse_n_states']):
    print(f"\nState {state}:")
    print(f"  Sample count: {int(state_analysis[f'State_{state}_Count'].iloc[0])}")
    
    # åˆ†æå…³é”®æŒ‡æ ‡
    state_mean = state_means[state]
    feature_dict = dict(zip(feature_cols, state_mean))
    
    # æŸ¥æ‰¾densityç›¸å…³çš„ç‰¹å¾ï¼ˆé€šå¸¸densityé«˜è¡¨ç¤ºæ²Ÿé€šé¢‘ç¹ï¼‰
    density_features = {k: v for k, v in feature_dict.items() if 'density' in k.lower()}
    if density_features:
        avg_density = np.mean(list(density_features.values()))
        print(f"  Mean Density: {avg_density:.4f} (high values may indicate frequent communication)")
    
    # æŸ¥æ‰¾clusteringç›¸å…³çš„ç‰¹å¾
    clustering_features = {k: v for k, v in feature_dict.items() if 'clustering' in k.lower()}
    if clustering_features:
        avg_clustering = np.mean(list(clustering_features.values()))
        print(f"  Mean Clustering: {avg_clustering:.4f} (high values may indicate tight collaboration)")
    
    # æŸ¥æ‰¾eigenvectorç›¸å…³çš„ç‰¹å¾
    eigenvector_features = {k: v for k, v in feature_dict.items() if 'eigenvector' in k.lower()}
    if eigenvector_features:
        avg_eigenvector = np.mean(list(eigenvector_features.values()))
        print(f"  Mean Eigenvector: {avg_eigenvector:.4f} (high values may indicate high influence)")

print("\n" + "="*80)
print("ğŸ’¡ Suggestions:")
print("  1. Review the above feature values to determine which state has lower values (may be 'low communication' state)")
print("  2. Determine which state has higher values (may be 'high communication' state)")
print("  3. Based on the analysis results, set HMM_STATE_MAPPING in the configuration below")
print("  4. For example: If state 0 is low communication and state 3 is high communication, you can set:")
print("     HMM_STATE_MAPPING = {0: 0, 1: 1, 2: 2, 3: 3}  # Keep 4 classes")
print("     Or map to 3 classes: {0: 0, 1: 0, 2: 1, 3: 2}  # Low->0, Medium->1, High->2")
print("="*80)

# ä¿å­˜çŠ¶æ€åˆ†æç»“æœ
state_analysis_path = REPORTS_DIR / "hmm_state_analysis.csv"
state_analysis.to_csv(state_analysis_path, encoding='utf-8')
print(f"\nâœ“ State analysis results saved to: {state_analysis_path}")
print("  You can open the CSV file to view detailed values of all features for each state")

# 3.2 ç»“åˆä»»åŠ¡æ€§èƒ½æ•°æ®åˆ¤æ–­å“ªäº›çŠ¶æ€éœ€è¦å¹²é¢„
print("\n" + "-"*80)
print("3.2 Analyze HMM States vs Task Performance (Determine Intervention Need)")
print("-"*80)

try:
    # åŠ è½½ä»»åŠ¡æ€§èƒ½æ•°æ®
    task_metrics = load_intermediate('task_metrics')
    
    # å°†HMMçŠ¶æ€ä¸ä»»åŠ¡æ€§èƒ½å…³è”
    # åˆ›å»ºDataFrameï¼šæ¯ä¸ªçª—å£å¯¹åº”ä¸€ä¸ªçŠ¶æ€å’Œç»„
    state_performance_df = pd.DataFrame({
        'group': train_data['group'].values,
        'window_idx': train_data['window_idx'].values,
        'state': states_train
    })
    
    # åˆå¹¶ä»»åŠ¡æ€§èƒ½æ•°æ®ï¼ˆæŒ‰ç»„ï¼‰
    if 'group' in task_metrics.columns:
        # å¦‚æœtask_metricsæœ‰groupåˆ—ï¼Œåˆå¹¶
        state_performance_df = state_performance_df.merge(
            task_metrics, on='group', how='left', suffixes=('', '_task')
        )
    else:
        # å¦‚æœæ²¡æœ‰groupåˆ—ï¼Œå°è¯•ç”¨ç´¢å¼•åŒ¹é…
        print("  Note: task_metrics doesn't have 'group' column, using index matching")
        task_metrics_indexed = task_metrics.reset_index()
        if 'group' in task_metrics_indexed.columns:
            state_performance_df = state_performance_df.merge(
                task_metrics_indexed, on='group', how='left', suffixes=('', '_task')
            )
    
    # åˆ†ææ¯ä¸ªçŠ¶æ€å¯¹åº”çš„ä»»åŠ¡æ€§èƒ½
    print("\n" + "="*80)
    print("Task Performance by HMM State (Lower performance may indicate need for intervention):")
    print("="*80)
    
    # æ‰¾å‡ºä»»åŠ¡æ€§èƒ½ç›¸å…³çš„åˆ—ï¼ˆå¦‚å®Œæˆæ—¶é—´ã€æ•ˆç‡ç­‰ï¼‰
    performance_cols = [col for col in state_performance_df.columns 
                        if any(keyword in col.lower() for keyword in 
                              ['time', 'duration', 'efficiency', 'completion', 'score', 'performance', 'quality'])]
    
    if len(performance_cols) > 0:
        print(f"\nFound {len(performance_cols)} performance-related columns:")
        for col in performance_cols[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  - {col}")
        
        # åˆ†ææ¯ä¸ªçŠ¶æ€çš„å¹³å‡æ€§èƒ½
        for state in range(HMM_CONFIG['coarse_n_states']):
            state_mask = state_performance_df['state'] == state
            state_data = state_performance_df[state_mask]
            
            if len(state_data) > 0:
                print(f"\nState {state} (æ ·æœ¬æ•°: {len(state_data)}):")
                
                # è®¡ç®—æ¯ä¸ªæ€§èƒ½æŒ‡æ ‡çš„å¹³å‡å€¼
                for col in performance_cols[:3]:  # åªåˆ†æå‰3ä¸ªæœ€é‡è¦çš„æŒ‡æ ‡
                    if col in state_data.columns and state_data[col].notna().sum() > 0:
                        avg_value = state_data[col].mean()
                        print(f"  Average {col}: {avg_value:.4f}")
                
                # å¦‚æœæœ‰å¤šç»„æ•°æ®ï¼Œè®¡ç®—ç»„å†…å¹³å‡
                if 'group' in state_data.columns:
                    unique_groups = state_data['group'].unique()
                    if len(unique_groups) > 1:
                        group_avgs = []
                        for g in unique_groups:
                            group_data = state_data[state_data['group'] == g]
                            if len(group_data) > 0 and len(performance_cols) > 0:
                                # å–ç¬¬ä¸€ä¸ªæ€§èƒ½æŒ‡æ ‡çš„å¹³å‡å€¼
                                perf_col = performance_cols[0]
                                if perf_col in group_data.columns and group_data[perf_col].notna().sum() > 0:
                                    group_avgs.append(group_data[perf_col].mean())
                        if len(group_avgs) > 0:
                            print(f"  Average performance across {len(unique_groups)} groups: {np.mean(group_avgs):.4f}")
    else:
        print("\nâš ï¸  Warning: No performance-related columns found in task_metrics")
        print("  Available columns:", list(state_performance_df.columns)[:10])
        print("\n  Alternative method: Analyze by feature values (low values = may need intervention)")
        print("  - States with low density/clustering/eigenvector may indicate low communication")
        print("  - States with low reciprocity may indicate poor collaboration")
    
    # åŸºäºç‰¹å¾å€¼å’Œæ€§èƒ½æ•°æ®ç»™å‡ºå»ºè®®
    print("\n" + "="*80)
    print("ğŸ’¡ Intervention Recommendation:")
    print("="*80)
    
    # è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„ç»¼åˆ"å¥åº·åº¦"åˆ†æ•°ï¼ˆåŸºäºå…³é”®ç‰¹å¾ï¼‰
    state_health_scores = {}
    for state in range(HMM_CONFIG['coarse_n_states']):
        state_mean = state_means[state]
        feature_dict = dict(zip(feature_cols, state_mean))
        
        # è®¡ç®—å…³é”®ç‰¹å¾çš„å¹³å‡å€¼ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        key_features_list = []
        for keyword in ['density', 'clustering', 'eigenvector', 'reciprocity']:
            features = {k: v for k, v in feature_dict.items() if keyword in k.lower()}
            if features:
                key_features_list.extend(list(features.values()))
        
        if len(key_features_list) > 0:
            health_score = np.mean(key_features_list)
            state_health_scores[state] = health_score
        else:
            state_health_scores[state] = 0.0
    
    # æŒ‰å¥åº·åº¦åˆ†æ•°æ’åº
    sorted_states = sorted(state_health_scores.items(), key=lambda x: x[1])
    
    print("\nStates ranked by collaboration health (lower = may need intervention):")
    for rank, (state, score) in enumerate(sorted_states, 1):
        intervention_level = "HIGH" if rank <= 2 else "LOW"
        print(f"  Rank {rank}: State {state} (health score: {score:.4f}) - Intervention need: {intervention_level}")
    
    print("\nğŸ“‹ Suggested interpretation:")
    print(f"  - States {sorted_states[0][0]} and {sorted_states[1][0]}: Likely need intervention (low collaboration)")
    print(f"  - States {sorted_states[2][0]} and {sorted_states[3][0]}: Likely no intervention needed (better collaboration)")
    print("\n  You can map states to intervention labels:")
    print(f"    - Need intervention (label 1): [{sorted_states[0][0]}, {sorted_states[1][0]}]")
    print(f"    - No intervention (label 0): [{sorted_states[2][0]}, {sorted_states[3][0]}]")
    
    # è‡ªåŠ¨ç”Ÿæˆ2åˆ†ç±»æ˜ å°„å»ºè®®
    intervention_states = [sorted_states[0][0], sorted_states[1][0]]
    no_intervention_states = [sorted_states[2][0], sorted_states[3][0]]
    binary_mapping = {}
    for state in range(4):
        if state in intervention_states:
            binary_mapping[state] = 1  # éœ€è¦å¹²é¢„
        else:
            binary_mapping[state] = 0  # ä¸éœ€è¦å¹²é¢„
    
    print(f"\nğŸ’¡ Suggested 2-class mapping (based on health scores):")
    print(f"    HMM_STATE_MAPPING = {binary_mapping}")
    print(f"    # State {sorted_states[0][0]}, {sorted_states[1][0]} -> 1 (need intervention)")
    print(f"    # State {sorted_states[2][0]}, {sorted_states[3][0]} -> 0 (no intervention)")
    print("="*80)
    
    # ä¿å­˜å»ºè®®çš„æ˜ å°„ä¾›åç»­ä½¿ç”¨
    suggested_binary_mapping = binary_mapping
    
except Exception as e:
    print(f"\nâš ï¸  Warning: Could not analyze task performance: {e}")
    print("  Falling back to feature-based analysis only")
    print("\n  Recommendation: Review the feature values in hmm_state_analysis.csv")
    print("  - States with lower density/clustering/eigenvector values likely need intervention")
    print("  - States with higher values likely don't need intervention")
    # å¦‚æœæ²¡æœ‰åˆ†æç»“æœï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„
    suggested_binary_mapping = {0: 1, 1: 1, 2: 0, 3: 0}  # é»˜è®¤ï¼šçŠ¶æ€0,1éœ€è¦å¹²é¢„ï¼ŒçŠ¶æ€2,3ä¸éœ€è¦

# 4. æ˜ å°„çŠ¶æ€åˆ°æ ‡ç­¾
print("\n" + "-"*80)
print("4. Map HMM States to Multi-class Labels")
print("-"*80)

# HMMå¤šåˆ†ç±»é…ç½®
# âš ï¸  é‡è¦ï¼šæ ¹æ®ä¸Šé¢çš„çŠ¶æ€åˆ†æç»“æœï¼Œè®¾ç½®çŠ¶æ€æ˜ å°„
# æ¨èä½¿ç”¨2åˆ†ç±»ï¼ˆéœ€è¦å¹²é¢„ vs ä¸éœ€è¦å¹²é¢„ï¼‰ï¼Œå› ä¸ºï¼š
# 1. æ•°æ®é‡æ›´é€‚åˆ2åˆ†ç±»
# 2. æ›´ç¬¦åˆå®é™…åº”ç”¨éœ€æ±‚
# 3. ç±»åˆ«æ›´å¹³è¡¡ï¼Œæ€§èƒ½æ›´å¥½

# è‡ªåŠ¨ä½¿ç”¨å»ºè®®çš„2åˆ†ç±»æ˜ å°„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    HMM_N_CLASSES = 2  # æ”¹ä¸º2åˆ†ç±»
    HMM_STATE_MAPPING = suggested_binary_mapping
    print(f"\nâœ“ Using suggested 2-class mapping based on state health analysis")
except NameError:
    # å¦‚æœæ²¡æœ‰åˆ†æç»“æœï¼Œä½¿ç”¨é»˜è®¤2åˆ†ç±»æ˜ å°„
    HMM_N_CLASSES = 2
    HMM_STATE_MAPPING = {0: 1, 1: 1, 2: 0, 3: 0}  # é»˜è®¤ï¼šçŠ¶æ€0,1éœ€è¦å¹²é¢„ï¼ŒçŠ¶æ€2,3ä¸éœ€è¦
    print(f"\nâš ï¸  Using default 2-class mapping (you can adjust based on state analysis)")

# å¦‚æœéœ€è¦ä½¿ç”¨å…¶ä»–åˆ†ç±»æ•°ï¼Œå¯ä»¥æ‰‹åŠ¨ä¿®æ”¹ï¼š
# HMM_N_CLASSES = 4  # 4åˆ†ç±»ï¼šç›´æ¥ä½¿ç”¨HMMçš„4ä¸ªçŠ¶æ€
# HMM_STATE_MAPPING = None
# 
# HMM_N_CLASSES = 3  # 3åˆ†ç±»
# HMM_STATE_MAPPING = {0: 0, 1: 0, 2: 1, 3: 2}  # çŠ¶æ€0,1->ç±»åˆ«0ï¼ˆä½æ²Ÿé€šï¼‰ï¼ŒçŠ¶æ€2->ç±»åˆ«1ï¼ˆä¸­ç­‰ï¼‰ï¼ŒçŠ¶æ€3->ç±»åˆ«2ï¼ˆé«˜æ²Ÿé€šï¼‰

print(f"\nUsing HMM states as {HMM_N_CLASSES}-class labels")
if HMM_N_CLASSES == 2:
    print(f"State mapping: {HMM_STATE_MAPPING}")
    print(f"  - Label 0: No intervention needed (States: {[s for s, l in HMM_STATE_MAPPING.items() if l == 0]})")
    print(f"  - Label 1: Intervention needed (States: {[s for s, l in HMM_STATE_MAPPING.items() if l == 1]})")
elif HMM_N_CLASSES == 3:
    print(f"State mapping: {HMM_STATE_MAPPING if HMM_STATE_MAPPING else 'Default mapping (0,1->0, 2->1, 3->2)'}")
elif HMM_N_CLASSES == 4:
    if HMM_STATE_MAPPING is None:
        print("Directly using HMM's 4 states (0,1,2,3) as 4 classes")
        print("âš ï¸  Note: Classes 0,1,2,3 have no preset semantics, need to understand their meanings based on the state analysis above")
    else:
        print(f"State mapping: {HMM_STATE_MAPPING}")

y_train = map_states_to_labels(states_train, n_classes=HMM_N_CLASSES, state_mapping=HMM_STATE_MAPPING)
# å¦‚æœ states_train_val ä¸ºç©ºï¼Œåˆ›å»ºç©ºæ•°ç»„
if len(states_train_val) > 0:
    y_train_val = map_states_to_labels(states_train_val, n_classes=HMM_N_CLASSES, state_mapping=HMM_STATE_MAPPING)
else:
    y_train_val = np.array([])
# å¦‚æœ states_val ä¸ºç©ºï¼ˆå·²åˆå¹¶åˆ°æµ‹è¯•é›†ï¼‰ï¼Œåˆ›å»ºç©ºæ•°ç»„
if len(states_val) > 0:
    y_val = map_states_to_labels(states_val, n_classes=HMM_N_CLASSES, state_mapping=HMM_STATE_MAPPING)
else:
    y_val = np.array([])
y_test = map_states_to_labels(states_test, n_classes=HMM_N_CLASSES, state_mapping=HMM_STATE_MAPPING)

print(f"\nLabel distribution ({HMM_N_CLASSES} classes):")
if HMM_N_CLASSES == 2:
    print(f"Training set - Label 0 (No intervention): {np.sum(y_train == 0)}, Label 1 (Need intervention): {np.sum(y_train == 1)}")
    print(f"Test set - Label 0 (No intervention): {np.sum(y_test == 0)}, Label 1 (Need intervention): {np.sum(y_test == 1)}")
    print(f"  Training set balance: {np.sum(y_train == 0)/len(y_train):.2%} vs {np.sum(y_train == 1)/len(y_train):.2%}")
    print(f"  Test set balance: {np.sum(y_test == 0)/len(y_test):.2%} vs {np.sum(y_test == 1)/len(y_test):.2%}")
elif HMM_N_CLASSES == 3:
    print(f"Training set - Label 0: {np.sum(y_train == 0)}, Label 1: {np.sum(y_train == 1)}, Label 2: {np.sum(y_train == 2)}")
    print(f"Test set - Label 0: {np.sum(y_test == 0)}, Label 1: {np.sum(y_test == 1)}, Label 2: {np.sum(y_test == 2)}")
elif HMM_N_CLASSES == 4:
    print(f"Training set - Label 0: {np.sum(y_train == 0)}, Label 1: {np.sum(y_train == 1)}, Label 2: {np.sum(y_train == 2)}, Label 3: {np.sum(y_train == 3)}")
    print(f"Test set - Label 0: {np.sum(y_test == 0)}, Label 1: {np.sum(y_test == 1)}, Label 2: {np.sum(y_test == 2)}, Label 3: {np.sum(y_test == 3)}")

# 5. ä¿å­˜ç»“æœ
print("\n" + "-"*80)
print("5. Save Results")
print("-"*80)

save_intermediate('states_train', states_train)
if len(states_train_val) > 0:
    save_intermediate('states_train_val', states_train_val)
else:
    save_intermediate('states_train_val', np.array([]))
# ä¿å­˜ç©ºçš„éªŒè¯é›†çŠ¶æ€ï¼ˆå·²åˆå¹¶åˆ°æµ‹è¯•é›†ï¼‰
if len(states_val) > 0:
    save_intermediate('states_val', states_val)
else:
    save_intermediate('states_val', np.array([]))
save_intermediate('states_test', states_test)

save_intermediate('y_train', y_train)
if len(y_train_val) > 0:
    save_intermediate('y_train_val', y_train_val)
else:
    save_intermediate('y_train_val', np.array([]))
# ä¿å­˜ç©ºçš„éªŒè¯é›†æ ‡ç­¾ï¼ˆå·²åˆå¹¶åˆ°æµ‹è¯•é›†ï¼‰
if len(y_val) > 0:
    save_intermediate('y_val', y_val)
else:
    save_intermediate('y_val', np.array([]))
save_intermediate('y_test', y_test)

# 6. ç”ŸæˆæŠ¥å‘Š
print("\n" + "-"*80)
print("6. Generate HMM Analysis Report")
print("-"*80)

report_lines = []
report_lines.append("=" * 60)
report_lines.append("HMM Modeling Report")
report_lines.append("=" * 60)
report_lines.append(f"\nHMM Configuration:")
report_lines.append(f"  Number of states: {HMM_CONFIG['coarse_n_states']}")
report_lines.append(f"  Iterations: {HMM_CONFIG['n_iter']}")
report_lines.append(f"  Covariance type: {HMM_CONFIG['covariance_type']}")
report_lines.append(f"\nState distribution:")
report_lines.append(f"  Training set: {pd.Series(states_train).value_counts().sort_index().to_dict()}")
report_lines.append(f"  Test set: {pd.Series(states_test).value_counts().sort_index().to_dict()}")
report_lines.append(f"\nState semantic analysis:")
report_lines.append(f"  Detailed state feature analysis saved to: hmm_state_analysis.csv")
report_lines.append(f"  Please check this file to understand feature values for each state and determine state meanings")
report_lines.append(f"\nLabel distribution ({HMM_N_CLASSES} classes):")
if HMM_N_CLASSES == 2:
    report_lines.append(f"  Training set - Label 0 (No intervention): {np.sum(y_train == 0)}, Label 1 (Need intervention): {np.sum(y_train == 1)}")
    report_lines.append(f"  Test set - Label 0 (No intervention): {np.sum(y_test == 0)}, Label 1 (Need intervention): {np.sum(y_test == 1)}")
    report_lines.append(f"  State mapping: {HMM_STATE_MAPPING}")
elif HMM_N_CLASSES == 3:
    report_lines.append(f"  Training set - Label 0: {np.sum(y_train == 0)}, Label 1: {np.sum(y_train == 1)}, Label 2: {np.sum(y_train == 2)}")
    report_lines.append(f"  Test set - Label 0: {np.sum(y_test == 0)}, Label 1: {np.sum(y_test == 1)}, Label 2: {np.sum(y_test == 2)}")
    report_lines.append(f"  State mapping: {HMM_STATE_MAPPING if HMM_STATE_MAPPING else 'Default mapping (0,1->0, 2->1, 3->2)'}")
elif HMM_N_CLASSES == 4:
    report_lines.append(f"  Training set - Label 0: {np.sum(y_train == 0)}, Label 1: {np.sum(y_train == 1)}, Label 2: {np.sum(y_train == 2)}, Label 3: {np.sum(y_train == 3)}")
    report_lines.append(f"  Test set - Label 0: {np.sum(y_test == 0)}, Label 1: {np.sum(y_test == 1)}, Label 2: {np.sum(y_test == 2)}, Label 3: {np.sum(y_test == 3)}")
    if HMM_STATE_MAPPING is not None:
        report_lines.append(f"  State mapping: {HMM_STATE_MAPPING}")

report_text = "\n".join(report_lines)
print(report_text)

with open(REPORTS_DIR / "hmm_analysis_report.txt", 'w', encoding='utf-8') as f:
    f.write(report_text)

print(f"\nâœ“ Report saved to {REPORTS_DIR / 'hmm_analysis_report.txt'}")

print("\n" + "="*80)
print("HMM modeling completed!")
print("="*80)
print("\nNext step: Run `02_supervised_feature_selection.py` for supervised feature selection")

